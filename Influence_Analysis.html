<!DOCTYPE html>
<html>
<head>
<style>
.ex1 {
  margin-left: 50px;
}

.ex2 {
  margin-left: 71px;
}

p {
  text-indent: 15px;
}


a:link, a:visited {
  
  color: blue;
  padding: 5px 10px;
  text-align: center;
  text-decoration: none;
  display: inline-block;
}

a:hover, a:active {
  background-color: yellow;
}
</style>
</head>
<body>

<h1 class="ex2"><b><center><b>Influence Analysis</b></center></b></h1>


<h3 class="ex2"><b> RESEARCH PAPERS: </b></h3>

<ol class="ex2">
<h4 style="text-indent: -20px;"><b> BIG PICTURE </b></h4>


	<li><a href="https://christophm.github.io/interpretable-ml-book/influential.html">Interpretable Machine Learning-A Guide for Making Black Box Models Explainable.</a></li>
			<p>Christoph Molnar.</p>
			<p>Lulu. com, 2020.</p>
	
	<li><a href="https://arxiv.org/pdf/1703.04730.pdf">Understanding black-box predictions via influence functions. </a></li>
		<p>Koh, Pang Wei, and Percy Liang.</p>
		<p>International Conference on Machine Learning. PMLR, 2017.</p>	
	
	<li><a href="https://link.springer.com/content/pdf/10.1186/1758-2946-6-32.pdf">The influence of negative training set size on machine learning-based virtual screening.</a></li>
		<p>Kurczab, Rafał, Sabina Smusz, and Andrzej J. Bojarski.</p>
		<p>Journal of cheminformatics 6.1 (2014): 1-9.</p>
		
	<li><a href="https://arxiv.org/pdf/2012.04207.pdf">Efficient Estimation of Influence of a Training Instance.</a></li>
		<p>Kobayashi, Sosuke, et al. </p>
		<p>Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing. 2020.</p>
		
	<li><a href="https://arxiv.org/pdf/2012.15781.pdf">FastIF: Scalable Influence Functions for Efficient Model Interpretation and Debugging.</a></li>
		<p>Guo, Han, et al. </p>
		<p>arXiv e-prints (2020): arXiv-2012.</p>

	<li><a href="https://arxiv.org/pdf/2009.09841.pdf">Finding influential instances for distantly supervised relation extraction.</a></li>
		<p>Wang, Zifeng, et al. </p>
		<p>arXiv preprint arXiv:2009.09841 (2020).</p>
  
	<li><a href="http://proceedings.mlr.press/v80/ilse18a/ilse18a.pdf">Attention-based deep multiple instance learning.</a></li>
		<p>Ilse, Maximilian, Jakub Tomczak, and Max Welling.</p>
		<p>International conference on machine learning. PMLR, 2018.</p>

	<li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8019879">Analyzing the training processes of deep generative models.</a></li>
		<p>Liu, Mengchen, et al.</p>
		<p>IEEE transactions on visualization and computer graphics 24.1 (2017): 77-87.</p>

	<li><a href="https://arxiv.org/pdf/2103.11807.pdf">Data Cleansing for Deep Neural Networks with Storage-efficient Approximation of Influence Functions.</a></li>
		<p>Suzuki, Kenji, Yoshiyuki Kobayashi, and Takuya Narihira. </p>
		<p>arXiv e-prints (2021): arXiv-2103.</p>

	<li><a href="https://www.sciencedirect.com/science/article/pii/S1574954119303498">Shapley additive explanations for NO2 forecasting.</a></li>
		<p>García, María Vega, and José L. Aznarte. </p>
		<p>Ecological Informatics 56 (2020): 101039.</p>

	<li><a href="https://link.springer.com/chapter/10.1007/978-3-030-28730-6_4">ILIME: Local and Global Interpretable Model-Agnostic Explainer of Black-Box Decision.</a></li>
		<p>ElShawi, Radwa, et al. </p>
		<p>European Conference on Advances in Databases and Information Systems. Springer, Cham, 2019.</p>

	<li><a href="https://arxiv.org/ftp/arxiv/papers/2101/2101.06232.pdf">Towards interpreting ML-based automated malware detection models: a survey.</a></li>
		<p>Lin, Yuzhou, and Xiaolin Chang.</p>
		<p>arXiv e-prints (2021): arXiv-2101.</p>
		
	<li><a href="https://escholarship.org/content/qt3tm600q3/qt3tm600q3.pdf"> Deep Contrastive Explanation. </a></li>
		<p>Feghahati, Seyedamirhossein.</p>
		<p>University of California, Riverside, 2020.</p>

	<li><a href="https://proceedings.neurips.cc/paper/2019/file/5f14615696649541a025d3d0f8e0447f-Paper.pdf"> Data Cleansing for Models Trained with SGD. </a></li>
		<p>Hara, Satoshi, Atsushi Nitanda, and Takanori Maehara.</p>
		<p>Advances in Neural Information Processing Systems 32 (2019).</p>

  </ol>
    
  
<h3 class="ex2"><b> TOOLS: </b></h3>

  <ol class="ex1">
	<li><a href="https://github.com/kohpangwei/influence-release">Influence Function</a><b>: </b>Understanding Black-box Predictions via Influence Functions <a href="https://arxiv.org/pdf/1703.04730.pdf">[Paper]</a></li>
	<li><a href="https://github.com/salesforce/fast-influence-functions">FASTIF</a><b>: </b>FastIF: Scalable Influence Functions for Efficient Model Interpretation and Debugging <a href="https://arxiv.org/pdf/2012.15781.pdf">[Paper]</a></li>
    <li><A href="https://github.com/sato9hara/sgd-influence">sgd-influence</a><b>: </b>Data Cleansing for Models Trained with SGD <A href="https://proceedings.neurips.cc/paper/2019/file/5f14615696649541a025d3d0f8e0447f-Paper.pdf">[Paper]</a></li>
  </ol>


</body>
</html>
